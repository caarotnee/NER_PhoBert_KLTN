{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPW5juLRNEuXYRJnPXz6gUQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caarotnee/NER_PhoBert_KLTN/blob/main/NER_PhoBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5xE3hQ3jdJP",
        "outputId": "a498c200-ed2f-48dc-f47a-af6b1c5bfaed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "  Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "  Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "  Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "  Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "  Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "  Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "  Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "  Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "  Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "  Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu124\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/torch-2.7.0.dev20250310%2Bcu124-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/torchvision-0.22.0.dev20250226%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/torchaudio-2.6.0.dev20250226%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.25.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-triton==3.2.0+git4b3bb1f8 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.2.0%2Bgit4b3bb1f8-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu124/torch-2.7.0.dev20250226%2Bcu124-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/nightly/cu124/nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/pytorch_triton-3.2.0%2Bgit4b3bb1f8-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.5/166.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu124/torchvision-0.22.0.dev20250226%2Bcu124-cp311-cp311-linux_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu124/torch-2.7.0.dev20250226%2Bcu124-cp311-cp311-manylinux_2_28_x86_64.whl (864.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.6/864.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/cu124/torchaudio-2.6.0.dev20250226%2Bcu124-cp311-cp311-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-triton, sympy, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0.dev20250226+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.25.1 nvidia-nvjitlink-cu12-12.4.127 pytorch-triton-3.2.0+git4b3bb1f8 sympy-1.13.3 torch-2.7.0.dev20250226+cu124 torchaudio-2.6.0.dev20250226+cu124 torchvision-0.22.0.dev20250226+cu124\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (12.4.127)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (12.4.127)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0.dev20250226+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Cài đặt lại đúng phiên bản PyTorch và CUDA phù hợp trên Colab\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip uninstall -y nvidia-cublas-cu12 nvidia-cudnn-cu12 nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-nvjitlink-cu12\n",
        "\n",
        "# Cài đúng bản torch + CUDA 12.4\n",
        "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124\n",
        "\n",
        "# Cài đúng các thư viện NVIDIA CUDA phụ trợ yêu cầu bởi torch 2.6.0+cu124\n",
        "!pip install \\\n",
        "  nvidia-cublas-cu12==12.4.5.8 \\\n",
        "  nvidia-cudnn-cu12==9.1.0.70 \\\n",
        "  nvidia-cuda-runtime-cu12==12.4.127 \\\n",
        "  nvidia-cuda-cupti-cu12==12.4.127 \\\n",
        "  nvidia-cuda-nvrtc-cu12==12.4.127 \\\n",
        "  nvidia-cufft-cu12==11.2.1.3 \\\n",
        "  nvidia-curand-cu12==10.3.5.147 \\\n",
        "  nvidia-cusolver-cu12==11.6.1.9 \\\n",
        "  nvidia-cusparse-cu12==12.3.1.170 \\\n",
        "  nvidia-nvjitlink-cu12==12.4.127\n",
        "\n",
        "# Cài đặt datasets\n",
        "!pip install -qq datasets\n",
        "\n",
        "# Kiểm tra phiên bản PyTorch\n",
        "!pip show torch -qq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải các file dữ liệu\n",
        "!gdown 1DuwYRftQjQmQcAR4FVPH-HvGuxGi4ist\n",
        "!gdown 11xZZfla8CDH54-EeUUdnAAoT2ummuEJh\n",
        "!gdown 1wVyhQkhAzwod2at7Ir3tRHbdCMOszzwg\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezAFdd4dkZkA",
        "outputId": "33482c89-3426-430e-f6c5-445b6a275cd5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DuwYRftQjQmQcAR4FVPH-HvGuxGi4ist\n",
            "To: /content/train_word.conll\n",
            "100% 1.42M/1.42M [00:00<00:00, 157MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11xZZfla8CDH54-EeUUdnAAoT2ummuEJh\n",
            "To: /content/test_word.conll\n",
            "100% 958k/958k [00:00<00:00, 138MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wVyhQkhAzwod2at7Ir3tRHbdCMOszzwg\n",
            "To: /content/dev_word.conll\n",
            "100% 628k/628k [00:00<00:00, 84.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# để đọc file định dạng CoNLL\n",
        "def read_conll(file_path):\n",
        "    sentences = []\n",
        "    sentence_labels = []\n",
        "    unique_labels = set()  # To collect unique labels\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        current_sentence_tokens = []\n",
        "        current_sentence_labels = []\n",
        "\n",
        "        for line in file:\n",
        "            line = line.strip()  # Remove leading/trailing whitespace, including '\\n'\n",
        "\n",
        "            # If it's an empty line, sentence boundary detected\n",
        "            if not line:\n",
        "                if current_sentence_tokens:  # Check if there's a sentence to append\n",
        "                    sentences.append(' '.join(current_sentence_tokens))\n",
        "                    sentence_labels.append(' '.join(current_sentence_labels))\n",
        "                current_sentence_tokens = []  # Reset for the next sentence\n",
        "                current_sentence_labels = []  # Reset for the next sentence\n",
        "            else:\n",
        "                line_parts = line.split()  # Split line into token and label\n",
        "                current_sentence_tokens.append(line_parts[0])\n",
        "\n",
        "                if len(line_parts) >= 2:\n",
        "                    current_sentence_labels.append(line_parts[1])\n",
        "                    unique_labels.add(line_parts[1])  # Add label to the set of unique labels\n",
        "                else:\n",
        "                    current_sentence_labels.append('O')  # Default to 'O' if no label provided\n",
        "\n",
        "    # Append the last sentence if the file doesn't end with an empty line\n",
        "    if current_sentence_tokens:\n",
        "        sentences.append(' '.join(current_sentence_tokens))\n",
        "        sentence_labels.append(' '.join(current_sentence_labels))\n",
        "\n",
        "    print(f\"Unique labels found: {unique_labels}\")\n",
        "    return sentences, sentence_labels\n",
        "\n",
        "# Load the datasets\n",
        "test_sentences, test_labels = read_conll('./test_word.conll')\n",
        "dev_sentences, dev_labels = read_conll('./dev_word.conll')\n",
        "train_sentences, train_labels = read_conll('./train_word.conll')\n",
        "\n",
        "# Now, test_sentences, test_labels, dev_sentences, dev_labels, train_sentences, and train_labels are arrays of strings\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STGQ_BPtkcaB",
        "outputId": "1fc72ad3-a1ac-4be2-c3bc-2afbb65dc1f3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels found: {'I-ORGANIZATION', 'B-GENDER', 'B-SYMPTOM_AND_DISEASE', 'B-AGE', 'B-PATIENT_ID', 'I-JOB', 'B-DATE', 'I-SYMPTOM_AND_DISEASE', 'O', 'B-ORGANIZATION', 'B-LOCATION', 'I-NAME', 'I-DATE', 'B-JOB', 'I-PATIENT_ID', 'I-LOCATION', 'B-NAME', 'B-TRANSPORTATION', 'I-AGE', 'I-TRANSPORTATION'}\n",
            "Unique labels found: {'I-ORGANIZATION', 'B-GENDER', 'B-SYMPTOM_AND_DISEASE', 'B-AGE', 'B-PATIENT_ID', 'I-JOB', 'B-DATE', 'I-SYMPTOM_AND_DISEASE', 'O', 'B-ORGANIZATION', 'B-LOCATION', 'I-NAME', 'I-DATE', 'B-JOB', 'I-PATIENT_ID', 'I-LOCATION', 'B-NAME', 'B-TRANSPORTATION', 'I-TRANSPORTATION'}\n",
            "Unique labels found: {'I-ORGANIZATION', 'B-GENDER', 'B-SYMPTOM_AND_DISEASE', 'B-AGE', 'B-PATIENT_ID', 'I-JOB', 'B-DATE', 'I-SYMPTOM_AND_DISEASE', 'O', 'B-ORGANIZATION', 'B-LOCATION', 'I-NAME', 'I-DATE', 'B-JOB', 'I-PATIENT_ID', 'I-LOCATION', 'B-NAME', 'B-TRANSPORTATION', 'I-AGE', 'I-TRANSPORTATION'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "px4vtd5pm_gF",
        "outputId": "567beffa-d847-495a-8424-47bbebb50ce0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bác_sĩ Trần_Thanh_Linh , từ Bệnh_viện Chợ_Rẫy chi_viện phụ_trách đơn_nguyên hồi_sức tích_cực , cho biết \" bệnh_nhân 416 \" vẫn đang duy_trì ECMO , thở máy , hiện xơ phổi rất nhiều .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "CoggeesHnAeh",
        "outputId": "dfbe293b-1bb7-4877-af37-6907b8f050c5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O O O O B-ORGANIZATION I-ORGANIZATION O O O O O O O O O O B-PATIENT_ID O O O O O O O O O O B-SYMPTOM_AND_DISEASE I-SYMPTOM_AND_DISEASE I-SYMPTOM_AND_DISEASE I-SYMPTOM_AND_DISEASE O'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Chuẩn bị dữ liệu từ sentences và labels\n",
        "def prepare_dataset(sentences, labels):\n",
        "    return {'tokens': sentences, 'labels': labels}\n",
        "\n",
        "train_dataset = prepare_dataset(train_sentences, train_labels)\n",
        "dev_dataset = prepare_dataset(dev_sentences, dev_labels)\n",
        "test_dataset = prepare_dataset(test_sentences, test_labels)\n",
        "\n",
        "# Chuyển đổi dữ liệu string thành mảng\n",
        "def process_string_to_array(dataset):\n",
        "    return {\n",
        "        'tokens': [sentence.split() for sentence in dataset['tokens']],\n",
        "        'labels': [label_seq.split() for label_seq in dataset['labels']]\n",
        "    }\n",
        "\n",
        "# Xử lý dữ liệu\n",
        "train_dataset = process_string_to_array(train_dataset)\n",
        "dev_dataset = process_string_to_array(dev_dataset)\n",
        "test_dataset = process_string_to_array(test_dataset)\n",
        "\n",
        "# Chuyển đổi thành Hugging Face Dataset\n",
        "train_dataset = Dataset.from_dict(train_dataset)\n",
        "dev_dataset = Dataset.from_dict(dev_dataset)\n",
        "test_dataset = Dataset.from_dict(test_dataset)\n",
        "\n",
        "# In kích thước của các dataset và mẫu để kiểm tra\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Dev dataset size: {len(dev_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"Train dataset sample:\", train_dataset[0])\n",
        "print(\"Dev dataset sample:\", dev_dataset[0])\n",
        "print(\"Test dataset sample:\", test_dataset[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOaa5-LEkf8S",
        "outputId": "f25fda0f-2461-4947-fe66-ba7781122455"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 5027\n",
            "Dev dataset size: 2000\n",
            "Test dataset size: 3000\n",
            "Train dataset sample: {'tokens': ['Đồng_thời', ',', 'bệnh_viện', 'tiếp_tục', 'thực_hiện', 'các', 'biện_pháp', 'phòng_chống', 'dịch_bệnh', 'COVID', '-', '19', 'theo', 'hướng_dẫn', 'của', 'Bộ', 'Y_tế', '.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O']}\n",
            "Dev dataset sample: {'tokens': ['Bác_sĩ', 'Nguyễn_Trung_Nguyên', ',', 'Giám_đốc', 'Trung_tâm', 'Chống', 'độc', ',', 'Bệnh_viện', 'Bạch_Mai', ',', 'cho', 'biết', 'bệnh_nhân', 'được', 'chuyển', 'đến', 'bệnh_viện', 'ngày', '7/3', ',', 'chẩn_đoán', 'ngộ_độc', 'thuốc', 'điều_trị', 'sốt_rét', 'chloroquine', '.'], 'labels': ['O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'B-SYMPTOM_AND_DISEASE', 'I-SYMPTOM_AND_DISEASE', 'O', 'O', 'O', 'O']}\n",
            "Test dataset sample: {'tokens': ['Từ', '24', '-', '7', 'đến', '31', '-', '7', ',', 'bệnh_nhân', 'được', 'mẹ', 'là', 'bà', 'H.T.P', '(', '47', 'tuổi', ')', 'đón', 'về', 'nhà', 'ở', 'phường', 'Phước_Hoà', '(', 'bằng', 'xe_máy', ')', ',', 'không', 'đi', 'đâu', 'chỉ', 'ra', 'Tạp_hoá', 'Phượng', ',', 'chợ', 'Vườn_Lài', ',', 'phường', 'An_Sơn', 'cùng', 'mẹ', 'bán', 'tạp_hoá', 'ở', 'đây', '.'], 'labels': ['O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'O', 'B-AGE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'B-JOB', 'I-JOB', 'O', 'O', 'O']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Define an Example class\n",
        "class Example:\n",
        "    def __init__(self, words, slot_labels, guid=None):\n",
        "        self.words = words\n",
        "        self.slot_labels = slot_labels\n",
        "        self.guid = guid\n",
        "\n",
        "# Step 6: Convert the dataset to Example objects\n",
        "def convert_to_examples(dataset):\n",
        "    return [\n",
        "        Example(words=tokens, slot_labels=labels, guid=i)\n",
        "        for i, (tokens, labels) in enumerate(zip(dataset['tokens'], dataset['labels']))\n",
        "    ]\n",
        "\n",
        "# Convert datasets into Example objects\n",
        "train_examples = convert_to_examples(train_dataset)\n",
        "dev_examples = convert_to_examples(dev_dataset)\n",
        "test_examples = convert_to_examples(test_dataset)\n"
      ],
      "metadata": {
        "id": "u5pQS8kznan-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import os"
      ],
      "metadata": {
        "id": "X6IFisSjniw0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Định nghĩa chuyển đổi từ examples thành features\n",
        "def convert_examples_to_features(\n",
        "    examples,\n",
        "    max_seq_len,\n",
        "    tokenizer,\n",
        "    pad_label_id=-100,\n",
        "    cls_token_segment_id=0,\n",
        "    pad_token_segment_id=0,\n",
        "    sequence_segment_id=0,\n",
        "    mask_padding_with_zero=True,\n",
        "):\n",
        "    # Lấy các token đặc biệt từ tokenizer\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    unk_token = tokenizer.unk_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for example_index, example in enumerate(examples):\n",
        "        if example_index % 400 == 0:\n",
        "            logger.info(f\"Processing example {example_index} of {len(examples)}\")\n",
        "\n",
        "        tokens = []\n",
        "        label_ids = []\n",
        "\n",
        "        for word, label in zip(example.words, example.slot_labels):\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "\n",
        "            if not word_tokens:\n",
        "                word_tokens = [unk_token]\n",
        "\n",
        "            tokens.extend(word_tokens)\n",
        "            label_id = label_map[label]\n",
        "            label_ids.extend([label_id] + [pad_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        special_tokens_count = 2\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:max_seq_len - special_tokens_count]\n",
        "            label_ids = label_ids[:max_seq_len - special_tokens_count]\n",
        "\n",
        "        tokens.append(sep_token)\n",
        "        label_ids.append(pad_label_id)\n",
        "        token_type_ids = [sequence_segment_id] * len(tokens)\n",
        "\n",
        "        tokens = [cls_token] + tokens\n",
        "        label_ids = [pad_label_id] + label_ids\n",
        "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        padding_length = max_seq_len - len(input_ids)\n",
        "        input_ids += [pad_token_id] * padding_length\n",
        "        attention_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
        "        token_type_ids += [pad_token_segment_id] * padding_length\n",
        "        label_ids += [pad_label_id] * padding_length\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=None,  # Không cần token_type_ids với PhoBERT\n",
        "                slot_labels_ids=label_ids,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "M_EOJWAqnj3p"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the label list (ensure that it includes all labels from your dataset)\n",
        "label_list = ['B-ORGANIZATION', 'B-TRANSPORTATION', 'B-JOB', 'I-PATIENT_ID', 'B-NAME', 'I-DATE', 'O', 'B-PATIENT_ID', 'I-AGE', 'I-JOB', 'B-DATE', 'I-TRANSPORTATION', 'B-SYMPTOM_AND_DISEASE', 'I-SYMPTOM_AND_DISEASE', 'B-GENDER', 'I-NAME', 'B-LOCATION', 'I-LOCATION', 'I-ORGANIZATION', 'B-AGE']\n",
        "\n",
        "# Create a mapping from label strings to integers\n",
        "label_map = {label: i for i, label in enumerate(label_list)}\n"
      ],
      "metadata": {
        "id": "nj3DCu2Qlblc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "FnrtolUnn2_d"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, slot_labels_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.slot_labels_ids = slot_labels_ids\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ],
      "metadata": {
        "id": "91SMfKFWn-s7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Dùng tokenizer của PhoBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
        "\n",
        "# Kiểm tra các token đặc biệt và ID của token pad\n",
        "tokenizer.cls_token, tokenizer.sep_token, tokenizer.unk_token, tokenizer.pad_token_id\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qqzhu5oodoQ",
        "outputId": "ec012b3a-81a7-4720-e78e-6631dea69a7c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<s>', '</s>', '<unk>', 1)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "print(\"CLS:\", tokenizer.cls_token)\n",
        "print(\"SEP:\", tokenizer.sep_token)\n",
        "print(\"UNK:\", tokenizer.unk_token)\n",
        "print(\"PAD:\", tokenizer.pad_token_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psADWhw2pTkw",
        "outputId": "38ec7dc5-7d8a-4d23-ecf1-d73acabaf8c0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLS: <s>\n",
            "SEP: </s>\n",
            "UNK: <unk>\n",
            "PAD: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer  # hoặc RobertaTokenizerFast cũng được\n",
        "\n",
        "# Khởi tạo tokenizer của PhoBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base', use_fast=True)\n",
        "\n",
        "# Đảm bảo thêm khoảng trắng đầu mỗi từ\n",
        "tokenizer.add_prefix_space = True\n",
        "\n",
        "# Thiết lập độ dài tối đa\n",
        "max_seq_len = 128\n",
        "\n",
        "# Convert examples sang features\n",
        "train_features = convert_examples_to_features(train_examples, max_seq_len, tokenizer)\n",
        "dev_features = convert_examples_to_features(dev_examples, max_seq_len, tokenizer)\n",
        "test_features = convert_examples_to_features(test_examples, max_seq_len, tokenizer)\n"
      ],
      "metadata": {
        "id": "x_uCyiUWpgvi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Định nghĩa lớp Dataset cho PhoBERT\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(feature.input_ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(feature.attention_mask, dtype=torch.long),\n",
        "            # PhoBERT không có token_type_ids\n",
        "            'labels': torch.tensor(feature.slot_labels_ids, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Chuyển features thành dataset cho huấn luyện và đánh giá\n",
        "train_dataset = NERDataset(train_features)\n",
        "dev_dataset = NERDataset(dev_features)\n",
        "test_dataset = NERDataset(test_features)\n"
      ],
      "metadata": {
        "id": "O_Cq6B3Hkii2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYWIiLt7klya",
        "outputId": "35367fc7-1270-4e53-8266-08ff3ae6871e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([   0, 1248,    4,  757,  194,  112,    9,  717, 2137, 3795, 9089, 6232,\n",
              "         1927,   31, 1195,   63, 1010,    7,  125, 1059,    5,    2,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'labels': tensor([-100,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6, -100,\n",
              "         -100,    6,    6,    6,    6,    6,    0,   18,    6, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100])}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "# Xác định số lượng nhãn (đảm bảo rằng số nhãn của bạn phù hợp với dataset)\n",
        "num_labels = len(label_list)  # Ví dụ: ['O', 'B-ORG', 'I-ORG', ...]\n",
        "\n",
        "# Tải mô hình PhoBERT cho tác vụ phân loại token\n",
        "model = AutoModelForTokenClassification.from_pretrained('vinai/phobert-base', num_labels=num_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1heYExCkoAk",
        "outputId": "08f5d65c-b348-4a1b-80b7-bf0e9a96ef29"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx17c71FreLv",
        "outputId": "c5ddd176-fcfe-4dc4-bf84-ebb073ee6849"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForTokenClassification, AutoTokenizer\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "from datasets import load_dataset  # Nếu bạn cần load dataset từ Huggingface\n",
        "\n",
        "# Tự động đăng nhập wandb\n",
        "os.environ[\"WANDB_API_KEY\"] = \"fb80f73fcb020dd331c4509a5851a96be928f490\"  # Thêm mã API của bạn ở đây\n",
        "\n",
        "# Load tokenizer và model PhoBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"vinai/phobert-base\", num_labels=len(label_list))\n",
        "\n",
        "# Tải dataset của bạn, nếu cần thiết\n",
        "# dataset = load_dataset('path_to_your_dataset')\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    true_preds, true_labels = [], []\n",
        "\n",
        "    for pred, label in zip(preds, labels):\n",
        "        tmp_preds, tmp_labels = [], []\n",
        "        for p_i, l_i in zip(pred, label):\n",
        "            if l_i != -100:\n",
        "                tmp_preds.append(label_list[p_i])\n",
        "                tmp_labels.append(label_list[l_i])\n",
        "        true_preds.append(tmp_preds)\n",
        "        true_labels.append(tmp_labels)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, true_preds),\n",
        "        \"recall\": recall_score(true_labels, true_preds),\n",
        "        \"f1\": f1_score(true_labels, true_preds),\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_phobert\",\n",
        "    do_train=True,\n",
        "    do_eval=True,  # Bật việc đánh giá\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=[\"wandb\"],  # Tích hợp với WandB\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,  # Dataset huấn luyện\n",
        "    eval_dataset=dev_dataset,  # Dataset đánh giá\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Huấn luyện và đánh giá\n",
        "trainer.train()  # Đây sẽ tự động thực hiện việc huấn luyện qua các epoch và tính toán metrics\n",
        "eval_metrics = trainer.evaluate()  # Đánh giá mô hình sau huấn luyện\n",
        "print(eval_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GVmctvK-ro5Q",
        "outputId": "06a07d33-0693-42a5-a44e-58bf47016c3a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-37-2a570de48a72>:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvkhlinh\u001b[0m (\u001b[33mvkhlinh-l\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250420_193235-2vy9lyhp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vkhlinh-l/huggingface/runs/2vy9lyhp' target=\"_blank\">./ner_phobert</a></strong> to <a href='https://wandb.ai/vkhlinh-l/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/vkhlinh-l/huggingface' target=\"_blank\">https://wandb.ai/vkhlinh-l/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/vkhlinh-l/huggingface/runs/2vy9lyhp' target=\"_blank\">https://wandb.ai/vkhlinh-l/huggingface/runs/2vy9lyhp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='945' max='945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [945/945 05:56, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.581300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.911000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.665900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.504000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.426400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.401600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.320700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.201900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.205600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.166600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.142300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.140500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.139700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.136800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.140200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.110800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.083400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.096400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.099300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.084000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.110600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.107800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.091200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.059200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.082100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.081700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.058400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.071100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.059600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.081000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.079600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.049100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.045000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.059200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.052700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.056000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.038400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.050200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.048400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.052600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.049500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.062300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.046300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.045100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.066500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.049800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.036000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.040800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.037500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.049500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.035600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.048400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.043900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.032100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.051800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.043200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.026900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.047700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.043500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.033900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.039600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.022800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.036000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.034400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.032200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.031500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.041300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.037600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.027500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.037400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.022500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.025400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.032200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.035800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.016800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.041300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.033000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.035500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.023500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.020100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.028000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.029800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.041200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.026400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.036400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.026100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.030900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.029200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.025600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.028100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:12]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.06501360982656479, 'eval_precision': 0.9435377730981838, 'eval_recall': 0.9603482920294709, 'eval_f1': 0.9518688176326097, 'eval_runtime': 13.0547, 'eval_samples_per_second': 153.202, 'eval_steps_per_second': 9.575, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./phobert-ner-finetuned\")\n",
        "tokenizer.save_pretrained(\"./phobert-ner-finetuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWL2uvlBsgdR",
        "outputId": "f69a7100-ca6a-465b-8e38-23b93f4116e5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./phobert-ner-finetuned/tokenizer_config.json',\n",
              " './phobert-ner-finetuned/special_tokens_map.json',\n",
              " './phobert-ner-finetuned/vocab.txt',\n",
              " './phobert-ner-finetuned/bpe.codes',\n",
              " './phobert-ner-finetuned/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sau khi huấn luyện xong, đánh giá mô hình trên test set\n",
        "\n",
        "# Tạo dataset từ file test (dựa vào NERDataset bạn đã định nghĩa từ trước)\n",
        "# Assuming label_map is the correct mapping, otherwise replace with the correct variable\n",
        "label_to_id = label_map\n",
        "test_dataset = NERDataset(test_features) # Modify this line to use test_features\n",
        "\n",
        "# Đánh giá mô hình trên test set\n",
        "test_metrics = trainer.evaluate(test_dataset)\n",
        "\n",
        "# In kết quả\n",
        "print(\"\\n====== Đánh giá trên TEST set ======\")\n",
        "print(f\"Loss: {test_metrics['eval_loss']:.4f}\")\n",
        "print(f\"Precision: {test_metrics['eval_precision']:.4f}\")\n",
        "print(f\"Recall: {test_metrics['eval_recall']:.4f}\")\n",
        "print(f\"F1 Score: {test_metrics['eval_f1']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "WblzL2adsk4e",
        "outputId": "1cad4c1f-0799-4d91-9bd7-7eb7090f34fa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 02:23]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Đánh giá trên TEST set ======\n",
            "Loss: 0.0755\n",
            "Precision: 0.9326\n",
            "Recall: 0.9501\n",
            "F1 Score: 0.9413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "# Lấy nhãn thực và nhãn dự đoán từ tập test\n",
        "predictions, labels, _ = trainer.predict(test_dataset)\n",
        "preds = predictions.argmax(-1)  # Chọn nhãn có xác suất cao nhất từ predictions\n",
        "\n",
        "true_labels = []\n",
        "true_preds = []\n",
        "\n",
        "# Duyệt qua từng cặp dự đoán và nhãn thực tế\n",
        "for pred, label in zip(preds, labels):\n",
        "    true_label = []\n",
        "    true_pred = []\n",
        "    # Duyệt qua từng cặp nhãn trong token\n",
        "    for p_i, l_i in zip(pred, label):\n",
        "        if l_i != -100:  # Lọc các nhãn không hợp lệ (-100 là nhãn bỏ qua)\n",
        "            true_label.append(label_list[l_i])  # Lấy nhãn thực tế\n",
        "            true_pred.append(label_list[p_i])   # Lấy nhãn dự đoán\n",
        "    true_labels.append(true_label)\n",
        "    true_preds.append(true_pred)\n",
        "\n",
        "# In báo cáo chi tiết\n",
        "print(\"\\n====== Báo cáo chi tiết theo từng thực thể ======\")\n",
        "print(classification_report(true_labels, true_preds, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "YKVohibys_MM",
        "outputId": "e8ae6a43-6dfc-406c-ae29-81347805ea2a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Báo cáo chi tiết theo từng thực thể ======\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "                AGE     0.9688    0.9637    0.9662       579\n",
            "               DATE     0.9785    0.9915    0.9850      1652\n",
            "             GENDER     0.9555    0.9804    0.9678       460\n",
            "                JOB     0.5640    0.6879    0.6198       173\n",
            "           LOCATION     0.9380    0.9511    0.9445      4435\n",
            "               NAME     0.8862    0.9308    0.9080       318\n",
            "       ORGANIZATION     0.8595    0.8962    0.8775       771\n",
            "         PATIENT_ID     0.9728    0.9855    0.9791      1995\n",
            "SYMPTOM_AND_DISEASE     0.8708    0.8838    0.8772      1136\n",
            "     TRANSPORTATION     0.9842    0.9689    0.9765       193\n",
            "\n",
            "          micro avg     0.9326    0.9501    0.9413     11712\n",
            "          macro avg     0.8978    0.9240    0.9102     11712\n",
            "       weighted avg     0.9340    0.9501    0.9419     11712\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Tải mô hình và tokenizer đã huấn luyện\n",
        "model = AutoModelForTokenClassification.from_pretrained('./phobert-ner-finetuned')\n",
        "tokenizer = AutoTokenizer.from_pretrained('./phobert-ner-finetuned')\n",
        "\n",
        "# Define the label list to map the labels\n",
        "label_list = ['B-ORGANIZATION', 'B-TRANSPORTATION', 'B-JOB', 'I-PATIENT_ID', 'B-NAME', 'I-DATE', 'O',\n",
        "              'B-PATIENT_ID', 'I-AGE', 'I-JOB', 'B-DATE', 'I-TRANSPORTATION', 'B-SYMPTOM_AND_DISEASE',\n",
        "              'I-SYMPTOM_AND_DISEASE', 'B-GENDER', 'I-NAME', 'B-LOCATION', 'I-LOCATION', 'I-ORGANIZATION',\n",
        "              'B-AGE']\n",
        "\n",
        "# Create a mapping from integer labels to label names\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "\n",
        "# Hàm nhận diện thực thể cho một câu nhập vào\n",
        "def predict_entities(text):\n",
        "    # Tiền xử lý câu đầu vào\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        # Dự đoán các nhãn\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Lấy chỉ số nhãn có xác suất cao nhất cho mỗi token\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Lấy nhãn dự đoán từ các chỉ số\n",
        "    predicted_labels = [id2label[label.item()] for label in predictions[0]]\n",
        "\n",
        "    # Tách từ và nhãn dự đoán\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # In kết quả dễ nhìn hơn\n",
        "    print(\"\\nTokens and Predicted Labels:\")\n",
        "    print(f\"{'Token':<15} {'Predicted Label'}\")\n",
        "    print(\"-\" * 40)\n",
        "    for token, label in zip(tokens, predicted_labels):\n",
        "        print(f\"{token:<15} {label}\")\n",
        "\n",
        "# Nhập câu vào để thử nghiệm\n",
        "text = input(\"Nhập câu để nhận dạng thực thể: \")\n",
        "predict_entities(text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6xisPz9vK3t",
        "outputId": "74e208eb-5f60-4a41-e016-985aa8ea8f77"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nhập câu để nhận dạng thực thể: Vào ngày 1 tháng 6 năm 2021, bệnh nhân 456 đã được chuyển đến Bệnh viện Chợ Rẫy.\n",
            "\n",
            "Tokens and Predicted Labels:\n",
            "Token           Predicted Label\n",
            "----------------------------------------\n",
            "<s>             O\n",
            "Vào             O\n",
            "ngày            O\n",
            "1               B-DATE\n",
            "tháng           I-DATE\n",
            "6               I-DATE\n",
            "năm             O\n",
            "20@@            B-DATE\n",
            "21@@            O\n",
            ",               O\n",
            "bệnh            O\n",
            "nhân            O\n",
            "456             B-PATIENT_ID\n",
            "đã              O\n",
            "được            O\n",
            "chuyển          O\n",
            "đến             O\n",
            "Bệnh            B-LOCATION\n",
            "viện            I-LOCATION\n",
            "Chợ             I-LOCATION\n",
            "R@@             I-LOCATION\n",
            "ẫ@@             I-LOCATION\n",
            "y.              I-LOCATION\n",
            "</s>            O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzwGGzJmxc5h",
        "outputId": "ca599883-6cdd-4324-9644-594d0cd7191b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Tải mô hình và tokenizer đã huấn luyện\n",
        "model = AutoModelForTokenClassification.from_pretrained('./phobert-ner-finetuned')\n",
        "tokenizer = AutoTokenizer.from_pretrained('./phobert-ner-finetuned')\n",
        "\n",
        "# Define the label list to map the labels\n",
        "label_list = ['B-ORGANIZATION', 'B-TRANSPORTATION', 'B-JOB', 'I-PATIENT_ID', 'B-NAME', 'I-DATE', 'O',\n",
        "              'B-PATIENT_ID', 'I-AGE', 'I-JOB', 'B-DATE', 'I-TRANSPORTATION', 'B-SYMPTOM_AND_DISEASE',\n",
        "              'I-SYMPTOM_AND_DISEASE', 'B-GENDER', 'I-NAME', 'B-LOCATION', 'I-LOCATION', 'I-ORGANIZATION',\n",
        "              'B-AGE']\n",
        "\n",
        "# Create a mapping from integer labels to label names\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "\n",
        "# Hàm nhận diện thực thể cho một câu nhập vào\n",
        "def predict_entities(text):\n",
        "    # Tiền xử lý câu đầu vào\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        # Dự đoán các nhãn\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Lấy chỉ số nhãn có xác suất cao nhất cho mỗi token\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Lấy nhãn dự đoán từ các chỉ số\n",
        "    predicted_labels = [id2label[label.item()] for label in predictions[0]]\n",
        "\n",
        "    # Tách từ và nhãn dự đoán\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # Chuẩn bị kết quả để hiển thị dễ nhìn\n",
        "    result = []\n",
        "    for token, label in zip(tokens, predicted_labels):\n",
        "        result.append(f\"{token:<15} {label}\")\n",
        "\n",
        "    return \"\\n\".join(result)\n",
        "\n",
        "\n",
        "# Tạo giao diện Gradio\n",
        "iface = gr.Interface(\n",
        "    fn=predict_entities,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Entity Recognition Demo\",\n",
        "    description=\"Nhập vào một câu để nhận diện các thực thể. Ví dụ: 'Bệnh nhân Nguyễn Văn A, 35 tuổi, đến từ Hà Nội.'\"\n",
        ")\n",
        "\n",
        "# Chạy giao diện\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "KjWBGtmxxmKK",
        "outputId": "c073a62d-603e-4322-bb3e-bf346663acfc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://345fda5be59eb5fa1d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://345fda5be59eb5fa1d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-nLkqUpzK6-",
        "outputId": "76ccc9b6-014d-4c4b-e5a4-3525474886eb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Đường dẫn đến thư mục mô hình đã huấn luyện\n",
        "model_dir = './phobert-ner-finetuned'\n",
        "\n",
        "# Đường dẫn lưu mô hình trong Google Drive\n",
        "drive_model_dir = '/content/drive/MyDrive/phobert-ner-finetuned'\n",
        "\n",
        "# Sao chép mô hình vào Google Drive\n",
        "shutil.copytree(model_dir, drive_model_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XVwBhbQ60S6R",
        "outputId": "7ecf899a-d6b3-4e1e-913b-50a4c5a8b98f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/phobert-ner-finetuned'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}